{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43d3be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\miasc\\SCH\\shinlab\\IBL\\VISp_PSTH\\main\n",
      "Python path: ['c:\\\\Users\\\\miasc\\\\anaconda3\\\\envs\\\\iblenv\\\\python39.zip', 'c:\\\\Users\\\\miasc\\\\anaconda3\\\\envs\\\\iblenv\\\\DLLs', 'c:\\\\Users\\\\miasc\\\\anaconda3\\\\envs\\\\iblenv\\\\lib', 'c:\\\\Users\\\\miasc\\\\anaconda3\\\\envs\\\\iblenv', '', 'c:\\\\Users\\\\miasc\\\\anaconda3\\\\envs\\\\iblenv\\\\lib\\\\site-packages', 'c:\\\\Users\\\\miasc\\\\anaconda3\\\\envs\\\\iblenv\\\\lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\miasc\\\\anaconda3\\\\envs\\\\iblenv\\\\lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\miasc\\\\anaconda3\\\\envs\\\\iblenv\\\\lib\\\\site-packages\\\\Pythonwin', '././fucn/', '././fucn/', '././fucn/']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm, colors\n",
    "from pprint import pprint\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from IPython.display import display\n",
    "\n",
    "# brainbox / iblatlas / ONE 관련\n",
    "from brainbox.io.one import SessionLoader, SpikeSortingLoader\n",
    "from brainbox.singlecell import bin_spikes\n",
    "from brainbox.ephys_plots import plot_brain_regions\n",
    "from iblatlas.atlas import AllenAtlas\n",
    "from one.api import ONE\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "def add_module_paths(base, *rel_paths):\n",
    "    for rel_path in rel_paths:\n",
    "        sys.path.append(os.path.join(base, *rel_path))\n",
    "\n",
    "add_module_paths(BASE_DIR,\n",
    "    ['func'],               # func 바로 아래 함수들\n",
    "    ['func', 'compute'],\n",
    "    ['func', 'info'],\n",
    "    ['func', 'plot']\n",
    ")\n",
    "\n",
    "from compute_raster import compute_raster\n",
    "from sub_func import save_file\n",
    "\n",
    "SAVE_PATH = r\"C:\\Users\\miasc\\SCH\\shinlab\\ChaeHyeon_Seong\\dataset_v2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d906408",
   "metadata": {},
   "outputs": [],
   "source": [
    "one = ONE()\n",
    "\n",
    "brain_acronym = 'VISp'\n",
    "sessions = list(one.search(atlas_acronym=brain_acronym, query_type='remote'))\n",
    "\n",
    "for i in [44, 45, 68, 68, 68]: sessions.pop(i)\n",
    "print(f\"Found {len(sessions)} sessions for region: {brain_acronym}\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 세션 반복\n",
    "# ---------------------------------------------------------------\n",
    "i = 1\n",
    "for eid in sessions:\n",
    "\n",
    "    if i == 10: break # 원하는 세션 수 만큼 반복 후 종료 : 10개\n",
    "\n",
    "    # Session ID\n",
    "    print(f\"\\n=== [Session:{i} {eid}] ===\"); i += 1\n",
    "\n",
    "    # Dataset 내에 Session 폴더 생성\n",
    "    save_session_path = os.path.join(SAVE_PATH, eid); os.makedirs(save_session_path, exist_ok=True)\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # Save (1) : Dataset 내에 ONE 폴더 내의 Session 위치 저장\n",
    "    # -----------------------------------------------------------------\n",
    "    \n",
    "    with open(os.path.join(save_session_path, \"session_path.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(str(one.eid2path(eid)))\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # Save (2) : Trials 정보 로드 & 저장\n",
    "    # -----------------------------------------------------------------\n",
    "    sl = SessionLoader(eid=eid, one=one)\n",
    "    sl.load_trials()\n",
    "    trials_df = sl.trials\n",
    "\n",
    "    # --- basic task와 full task 구분 ---\n",
    "    if 'probabilityLeft' in trials_df.columns:\n",
    "        unique_prob = np.unique(trials_df['probabilityLeft'].dropna()) \n",
    "        if len(unique_prob) == 1 and np.isclose(unique_prob[0], 0.5): \n",
    "            task_type = 'basic'\n",
    "        else:\n",
    "            task_type = 'full'\n",
    "    else:\n",
    "        task_type = 'unknown'\n",
    "    print(f\"Session {eid} is identified as a {task_type} task.\")\n",
    "    with open(os.path.join(save_session_path, \"task_type.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(task_type)\n",
    "\n",
    "    trials_df_selected = trials_df[['stimOn_times', 'contrastLeft', 'contrastRight',\n",
    "                                    'choice', 'feedbackType', 'response_times']].copy()\n",
    "    trials_df_selected['task_type'] = task_type\n",
    "\n",
    "    save_file(trials_df_selected,\n",
    "              save_path=save_session_path,\n",
    "              save_title=\"trials_info\")\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # 2) Spike/Cluster 정보: Probe 별로 반복\n",
    "    # -----------------------------------------------------------------\n",
    "    pids, labels = one.eid2pid(eid)\n",
    "    if len(pids) == 0:\n",
    "        print(f\" - No probe data found in session {eid}, skip.\")\n",
    "        continue\n",
    "\n",
    "    events = sl.trials['stimOn_times'].values\n",
    "\n",
    "    for pid, label in zip(pids, labels):\n",
    "        print(f\"   -> Probe: {pid} ({label})\")\n",
    "        probe_folder = os.path.join(save_session_path, label)\n",
    "        os.makedirs(probe_folder, exist_ok=True)\n",
    "\n",
    "        clusters_from_obj = one.load_object(eid, 'clusters', collection=f'alf/{label}/pykilosort')\n",
    "        peak2trough = clusters_from_obj.peakToTrough\n",
    "\n",
    "        ssl = SpikeSortingLoader(one=one, pid=pid, atlas=AllenAtlas())\n",
    "        spikes, clusters, channels = ssl.load_spike_sorting()\n",
    "        clusters = ssl.merge_clusters(spikes, clusters, channels)\n",
    "\n",
    "        # -----------------------------\n",
    "        # 2-1) 뉴런 정보와 spike waveform 분류 및 전체 recording FR 계산\n",
    "        # -----------------------------\n",
    "        clusters_df = clusters.to_df()\n",
    "        clusters_df['peakToTrough'] = peak2trough\n",
    "        threshold = 0.5  # 임계값 (ms)\n",
    "        clusters_df['spikeType'] = clusters_df['peakToTrough'].abs().apply(\n",
    "            lambda x: 'fast-spiking' if x < threshold else 'regular-spiking'\n",
    "        )\n",
    "\n",
    "        # 전체 recording에서 각 뉴런의 평균 firing rate 계산\n",
    "        # spikes.times는 session 전체의 spike 시간 (초 단위)\n",
    "        # 각 unit에 대해 spike 개수를 세고, 전체 recording 기간(마지막 - 첫 spike 시간)으로 나눔\n",
    "        recording_duration = spikes.times[-1] - spikes.times[0]\n",
    "        unique_units = np.unique(spikes.clusters)\n",
    "        mean_fr_dict = {unit: np.sum(spikes.clusters == unit) / recording_duration \n",
    "                        for unit in unique_units}\n",
    "        # clusters_df의 인덱스 혹은 'id' 컬럼을 사용하여 매핑 (여기서는 index를 사용)\n",
    "        clusters_df['meanFR_session'] = clusters_df.index.map(lambda x: mean_fr_dict.get(x, np.nan))\n",
    "\n",
    "        save_file(clusters_df, save_path=probe_folder, save_title=\"neuron_info\")\n",
    "        # 전체 session FR를 별도로 저장 (Method 1용)\n",
    "        np.save(os.path.join(probe_folder, \"mean_fr_session.npy\"), \n",
    "                clusters_df['meanFR_session'].values)\n",
    "\n",
    "        # -------------------------------------------------------------\n",
    "        # 2-2) PSTH 계산 (trial window 기반은 그대로 두되, Method 2, 3 계산은 그대로 수행)\n",
    "        # -------------------------------------------------------------\n",
    "        pre_time = 2.0\n",
    "        post_time = 4.0\n",
    "        bin_size = 0.001  # 1ms\n",
    "        # psth의 시간 배열 (-2 ~ +4초)\n",
    "        # (참고: 이 PSTH는 trial-locked window로 계산됨)\n",
    "        # times = np.arange(-pre_time, post_time, bin_size)  <- 이미 compute_raster 내부에서 사용됨\n",
    "\n",
    "        spike_raster, time_bins = compute_raster(spikes, \n",
    "                                                  np.where((clusters_df.index.values)[clusters_df.index.isin(unique_units)])[0],\n",
    "                                                  events,\n",
    "                                                  pre_time=pre_time,\n",
    "                                                  post_time=post_time,\n",
    "                                                  bin_size=bin_size)\n",
    "        save_file(spike_raster, save_path=probe_folder, save_title=\"psth\")\n",
    "        save_file(time_bins, save_path=probe_folder, save_title=\"time_bins\")\n",
    "\n",
    "print(\"\\n\\nAll done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iblenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
