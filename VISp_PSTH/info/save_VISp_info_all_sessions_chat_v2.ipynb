{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43d3be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\miasc\\SCH\\shinlab\\IBL\\VISp_PSTH\\main\n",
      "Python path: ['c:\\\\Users\\\\miasc\\\\anaconda3\\\\envs\\\\iblenv\\\\python39.zip', 'c:\\\\Users\\\\miasc\\\\anaconda3\\\\envs\\\\iblenv\\\\DLLs', 'c:\\\\Users\\\\miasc\\\\anaconda3\\\\envs\\\\iblenv\\\\lib', 'c:\\\\Users\\\\miasc\\\\anaconda3\\\\envs\\\\iblenv', '', 'c:\\\\Users\\\\miasc\\\\anaconda3\\\\envs\\\\iblenv\\\\lib\\\\site-packages', 'c:\\\\Users\\\\miasc\\\\anaconda3\\\\envs\\\\iblenv\\\\lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\miasc\\\\anaconda3\\\\envs\\\\iblenv\\\\lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\miasc\\\\anaconda3\\\\envs\\\\iblenv\\\\lib\\\\site-packages\\\\Pythonwin', '././fucn/', '././fucn/', '././fucn/']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm, colors\n",
    "from pprint import pprint\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from IPython.display import display\n",
    "\n",
    "# brainbox / iblatlas / ONE 관련\n",
    "from brainbox.io.one import SessionLoader, SpikeSortingLoader\n",
    "from brainbox.singlecell import bin_spikes\n",
    "from brainbox.ephys_plots import plot_brain_regions\n",
    "from iblatlas.atlas import AllenAtlas\n",
    "from one.api import ONE\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "def add_module_paths(base, *rel_paths):\n",
    "    for rel_path in rel_paths:\n",
    "        sys.path.append(os.path.join(base, *rel_path))\n",
    "\n",
    "add_module_paths(BASE_DIR,\n",
    "    ['func'],               # func 바로 아래 함수들\n",
    "    ['func', 'compute'],\n",
    "    ['func', 'info'],\n",
    "    ['func', 'plot']\n",
    ")\n",
    "\n",
    "from compute_raster import compute_raster\n",
    "from sub_func import save_file\n",
    "from get_task_type import get_task_type\n",
    "\n",
    "SAVE_PATH = r\"C:\\Users\\miasc\\SCH\\shinlab\\ChaeHyeon_Seong\\dataset_v2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d906408",
   "metadata": {},
   "outputs": [],
   "source": [
    "one = ONE()\n",
    "\n",
    "brain_acronym = 'VISp'\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# < Session 별로 정보 저장 >\n",
    "\n",
    "# - 각 세션에 대해, 세션 폴더를 생성. 폴더명은 Session ID\n",
    "\n",
    "# - 세션 폴더 내에, 저장하는 정보:\n",
    "#  1) session_path.txt : 해당 세션의 ONE 경로 정보\n",
    "#  2) task_type.txt : 해당 세션의 task type\n",
    "#  3) trials_info.csv : 해당 세션의 trials 정보\n",
    "#     (stimOn_times, contrastLeft, contrastRight, choice, feedbackType, response_times)\n",
    "#     (task_type도 포함)\n",
    "\n",
    "    # < Probe 별로 정보 저장 >\n",
    "\n",
    "    # - 세션 폴더 내에, 각 프로브 별로 폴더를 생성. 폴더명은 probe label\n",
    "\n",
    "    # - 각 프로브 폴더 내에, 저장하는 정보:\n",
    "    #  1) mean_fr_session.npy : 해당 프로브의 전체 recording에서의 평균 firing rate\n",
    "    #  2) neuron_info.csv : 해당 프로브의 뉴런 정보 (spike waveform, firing rate 등)\n",
    "    #  3) psth.npy : 해당 프로브의 PSTH 정보 (spike raster)\n",
    "    #  4) time_bins.npy : PSTH의 시간 bin 정보\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "# << Session ID list >>\n",
    "sessions = list(one.search(atlas_acronym=brain_acronym, query_type='remote'))\n",
    "\n",
    "for i in [44, 45, 68, 68, 68]: sessions.pop(i)\n",
    "print(f\"Found {len(sessions)} sessions for region: {brain_acronym}\")\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# {1} : Session 별 정보 저장\n",
    "# -----------------------------------------------------------------\n",
    "i = 1 # Session 번호 초기화\n",
    "\n",
    "for eid in sessions:\n",
    "\n",
    "    if i == 10: break # i = n : 원하는 세션 수 n 만큼 저장 후 종료\n",
    "\n",
    "    # --- Session ID (EID) ---\n",
    "    print(f\"\\n=== [Session:{i} {eid}] ===\"); i += 1\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # {1}-(1) : Dataset 내에 Session 폴더 생성 (폴더명: Session ID)\n",
    "    # -----------------------------------------------------------------\n",
    "    SAVE_SESSION_PATH = os.path.join(SAVE_PATH, eid); os.makedirs(SAVE_SESSION_PATH, exist_ok=True)\n",
    "\n",
    "    # [ Save 1 ] : session_path.txt - 해당 세션의 ONE 경로 정보 저장\n",
    "    \n",
    "    with open(os.path.join(SAVE_SESSION_PATH, \"session_path.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(str(one.eid2path(eid)))\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # {1}-(2) : Task type (Basic task or Full task) 판별\n",
    "    # -----------------------------------------------------------------\n",
    "    task_type = get_task_type(trials_df, print_info=True)\n",
    "\n",
    "    # [ Save 2 ] : task_type.txt - 해당 세션의 task type 저장\n",
    "    with open(os.path.join(SAVE_SESSION_PATH, \"task_type.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(task_type)\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # {1}-(3) : Trials 정보 저장\n",
    "    # -----------------------------------------------------------------\n",
    "    sl = SessionLoader(eid=eid, one=one)\n",
    "    sl.load_trials()\n",
    "    trials_df = sl.trials\n",
    "    trials_df = trials_df[['stimOn_times', 'contrastLeft', 'contrastRight',\n",
    "                                    'choice', 'feedbackType', 'response_times']]\n",
    "    trials_df['task_type'] = task_type\n",
    "\n",
    "    # [ Save 3 ] : Trials 정보 저장\n",
    "    save_file(trials_df, save_path=SAVE_SESSION_PATH, save_title=\"trials_info\")\n",
    "\n",
    "\n",
    "# << Probe ID list >>\n",
    "\n",
    "    pids, labels = one.eid2pid(eid)\n",
    "    if len(pids) == 0: \n",
    "        print(f\" - No probe data found in session {eid}, skip.\"); continue # No probe data found\n",
    "    \n",
    "    # -----------------------------------------------------------------\n",
    "    # {2} : Probe 별 정보 저장\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    for pid, label in zip(pids, labels):\n",
    "\n",
    "        # --- Prove ID (PID) ---\n",
    "        print(f\"   ===> [Probe: {pid} ({label})]\")\n",
    "\n",
    "        # -----------------------------------------------------------------\n",
    "        # {2}-(1) : Session 폴더 내에 Prove 별 폴더 생성 (폴더명: probe label)\n",
    "        # -----------------------------------------------------------------\n",
    "        SAVE_PROBE_PATH = os.path.join(SAVE_SESSION_PATH, label); os.makedirs(SAVE_PROBE_PATH, exist_ok=True)\n",
    "\n",
    "\n",
    "        # -------------------------------------------------------------\n",
    "        # {2}-(2) : Spike Sorting 정보 (spikes, clusters, channels)\n",
    "        # -------------------------------------------------------------\n",
    "        ssl = SpikeSortingLoader(one=one, pid=pid, atlas=AllenAtlas())\n",
    "        spikes, clusters, channels = ssl.load_spike_sorting()\n",
    "        clusters_from_ssl = ssl.merge_clusters(spikes, clusters, channels)\n",
    "        clusters_df = clusters_from_ssl.to_df()\n",
    "\n",
    "\n",
    "        # -------------------------------------------------------------\n",
    "        # {2}-(3) : Neuron별 mean Firing Rate 계산 (전체 recording 구간에서의 평균 FR)\n",
    "        # -------------------------------------------------------------\n",
    "    \n",
    "        recording_duration = spikes.times[-1] - spikes.times[0] # 전체 recording의 시간 (마지막 - 첫 spike 시간) (초 단위)\n",
    "        # 각 unit에 대해 spike 개수를 세고, 전체 recording 기간(마지막 - 첫 spike 시간)으로 나눔\n",
    "        unique_units = np.unique(spikes.clusters)\n",
    "        mean_fr_dict = {unit: np.sum(spikes.clusters == unit) / recording_duration \n",
    "                        for unit in unique_units}\n",
    "        clusters_df['meanFR'] = clusters_df.index.map(lambda x: mean_fr_dict.get(x, np.nan))\n",
    "\n",
    "        # [ Save 4 ] : meanFR 정보 저장 (Method 1용)\n",
    "        np.save(os.path.join(SAVE_PROBE_PATH, \"meanFR.npy\"), clusters_df['meanFR'].values)\n",
    "\n",
    "        # -------------------------------------------------------------\n",
    "        # {2}-(4) : Spike Type 정보 (peakToTrough, fast-spiking vs regular-spiking)\n",
    "        # -------------------------------------------------------------\n",
    "        clusters_from_obj = one.load_object(eid, 'clusters', collection=f'alf/{label}/pykilosort')\n",
    "\n",
    "        # peakToTrough 값이 있는 경우, spikeType을 계산하여 pekToTrough 값과 함께 clusters_df에 추가\n",
    "        if 'peakToTrough' in clusters_from_obj:\n",
    "            peak2trough = clusters_from_obj['peakToTrough']\n",
    "\n",
    "            # spikeType 계산: peakToTrough 값이 임계값(0.5ms)보다 작으면 'fast-spiking', 그렇지 않으면 'regular-spiking'\n",
    "            clusters_df['peakToTrough'] = peak2trough\n",
    "            threshold = 0.5  # 임계값 (ms)\n",
    "            clusters_df['spikeType'] = clusters_df['peakToTrough'].abs().apply(\n",
    "                lambda x: 'fast-spiking' if x < threshold else 'regular-spiking'\n",
    "            )\n",
    "\n",
    "        else: # 만약 없다면, NaN으로 처리, spikeType를 'unknown'으로 설정\n",
    "            print(f\" - No peakToTrough data found for probe {pid}, setting to NaN.\")\n",
    "            peak2trough = np.full(len(clusters_df), np.nan)\n",
    "            clusters_df['peakToTrough'] = peak2trough # NaN으로 설정\n",
    "            clusters_df['spikeType'] = 'unknown'  # spikeType을 'unknown'으로 설정  \n",
    "\n",
    "\n",
    "        # [ Save 5 ] : neuron_info.csv - cluster_df 정보 저장 (meanFR, peakToTrough, spikeType 등 포함)\n",
    "        save_file(clusters_df, save_path=SAVE_PROBE_PATH, save_title=\"neuron_info\")\n",
    "\n",
    "        # -------------------------------------------------------------\n",
    "        # {2}-(4) PSTH 계산 (trial window 기반은 그대로 두되, Method 2, 3 계산은 그대로 수행)\n",
    "        # -------------------------------------------------------------\n",
    "    \n",
    "        pre_time = 2.0\n",
    "        post_time = 4.0\n",
    "        bin_size = 0.001  # 1ms\n",
    "        # psth의 시간 배열 (-2 ~ +4초)\n",
    "        # (참고: 이 PSTH는 trial-locked window로 계산됨)\n",
    "        # times = np.arange(-pre_time, post_time, bin_size)  <- 이미 compute_raster 내부에서 사용됨\n",
    "\n",
    "        spike_raster, time_bins = compute_raster(spikes, \n",
    "                                                  np.where((clusters_df.index.values)[clusters_df.index.isin(unique_units)])[0],\n",
    "                                                  events=sl.trials['stimOn_times'].values,\n",
    "                                                  pre_time=pre_time,\n",
    "                                                  post_time=post_time,\n",
    "                                                  bin_size=bin_size)\n",
    "        \n",
    "        # spike_raster는 (num_units, num_bins) 형태의 2D 배열\n",
    "        # time_bins는 (num_bins,) 형태의 1D 배열\n",
    "        # spike_raster의 각 unit에 대해, firing rate를 계산\n",
    "        \n",
    "        # [ Save 6 ] : PSTH 정보 저장 - spike_raster와 time_bins\n",
    "        save_file(spike_raster, save_path=SAVE_PROBE_PATH, save_title=\"psth\")\n",
    "        save_file(time_bins, save_path=SAVE_PROBE_PATH, save_title=\"time_bins\")\n",
    "\n",
    "print(\"\\n\\nAll done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iblenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
